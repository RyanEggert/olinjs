#Class 8 - Unit Testing and Task Running

##What is Unit Testing?

Quoth [Wikipedia](http://en.wikipedia.org/wiki/Unit_testing), "unit testing is a software testing method by which individual units of source code, sets of one or more computer program modules together with associated control data, usage procedures, and operating procedures, are tested to determine whether they are fit for use." Really what that means is that each piece of functionality gets a test to see if it works in isolation from all other pieces of functionality. +

Most of you have probably have heard the words "unit testing" before and probably have some association with lots of print statements to see if things are equal, and really it just takes a lot of time and isn't that important. Not so. 

##Why Unit Test?

Obviously unit testing helps catch bugs and make sure your code works, but it also provides other benefits in terms of overall code quality. Having a good test suite for a project increases maintainability as it becomes harder to inroduce new bugs in new versions without breaking tests. Additionally, high quality code tends to be very well encapsulated, and well encapsulated code tends to be easier to test. That said, when you sit down to test something and realize that the test is going to be complicated to write, consider first refactoring the code you are testing so you end up with several tests that are all easier to write, and better code in general as a result. 

##Test Driven Development (TDD)

A common development workflow is to write a bunch of code and then test that code. TDD turns that workflow on its head, calling instead for each piece of functionality to already have tests for it before implementing it. This approach has several benefits, most notably that tests get written for everything during the process of development, not as an afterthoguht. Additionally, writing tests before functionality effectively specifies interface for the code you have not written yet, so be the time you need to implement functionality, you already have a very clear specification for what that code should look like. This ties developers to functionality requirements better than just letting them write whatever, so you end up with less code that you do not need. TDD does have the disadvantage of requiring frequent refactoring, as implementing one feature at a time focusing on passing tests probably won't lead to very well organized code. That said, there are many more pros and cons of TDD outside the scope of this class. We will not require you to use TDD practices, but we recommend trying them out at some point. If you want to read more on TDD, [Wikipedia](http://en.wikipedia.org/wiki/Test-driven_development) is a good place to start.

##Unit Testing Fundamentals

There are several terms used in association with testing that are useful to understand:

**Test Suite**: A test suite consists of all the actual tests for your functionality. This is where the bulk of the work for somebody writing tests goes.

**Test Harness/Framework**: A testing framework is the high level code that defines how to write and run tests. Essentially this is what moves testing from 'run a bunch of code and look on the command line to see if it outputted the right thing' to something that does all of that comparison work for you, sorts through what's right and what's wrong, and tells you 'these all passed but this one failed and here's why'. You can write these yourself, but there are so many good ones to choose from. We will be using [Mocha](http://mochajs.org/), because it's fairly common and does both node and browser testing, but if you want to take a look at other frameworks, Wikipedia has a fairly comprehensive [list](http://en.wikipedia.org/wiki/List_of_unit_testing_frameworks#JavaScript)

**Test Runner**: A test runner runs your tests for you so you don't have to yourself. You tell it where your tests are and you get to run them all with just one command. We will use [Karma](http://karma-runner.github.io/0.12/index.html) for client-side tests. Karma's main benifit is that for testing client-side code, it deals with running your code in various browser environments so you can test for cross-browser compatability. For server-side code, mocha's built in runner is good enough. In the end, we will run the comman line interface of our tests through npm, but we will get to that a little later. 

**Asserion Library**: Everybody likes to write their tests differently, so people made assertion libraries. An assertion library simply defines the interface for doing comparison tests. Mocha supports tons of these, and [Chai](http://chaijs.com/) is fairly popular and versatile and can be installed through npm, so we will use it. 

**Continous Integration (CI)**: We aren't going to deal with CI today, but it may come up during project time. CI goes one step past a test runner and runs your tests for you automatically whenever relevant code changes. Check out [Travis CI](http://docs.travis-ci.com/) if you are interested. 

**Code Coverage**: Pretty straightforward, code coverage tells you how much of your code actually gets run when your test suite runs, so you know if you need to write more tests. It's extraordinarily easy to set up and quite useful. We will be using [Istanbul](https://gotwarlost.github.io/istanbul/) and [Istanbul with Karma](http://karma-runner.github.io/0.8/config/coverage.html).

The client-server divide causes serious problems for javascript testers. A problem that we do not have because we are working in nodejs is that most servers aren't even written in javascript. Additionally, server-client communications rely on network connectivity, and we really don't want our tests to care about whether the network is working. The same applies to any database interactions the server might execute. How then, do we test code that relies on sending out a request and receiving a response? Consider, for example, testing one of your client-side ajax requests and the callback it executes. For that test, you don't really care what the server is doing as long as it responds correctly, and you will be testing the server code separately anyway, so ideally you would fake the ajax call and just go straight to having the correct response without ever talking to the server at all. Turns out, you can do that, using [Sinon.js](http://sinonjs.org/), which provides several useful mocks of things like Ajax and XHR requests, and even the passage of time. The general idea is that any request-response interface can be faked, and Sinon provides a library to do that.

As you can see, there are tons of options for what to use for your javascript testing environment. We'll be going onwards with our recommendations, but they are just that, and any time you start a new project, it's a good idea to survey your testing options to see what will best fit your project.

##Getting Started with Mocha and Chai for Server-Side Testing

That was a lot of high-level explanation of what we are trying to do, why, and what tools we have for it, so now it's time to get into setting up our testing environment, starting with server-side tests, which are generally simpler. There's a very basic app set up in the `in_class` subdirectory, so let's go in there and check it out. From that directory, run `npm install` to get all your dependencies, then start by seeing what the app actually does, with `node app.js`. As you can see, it's very, very basic, and it just tells you hello, but it is a working express app. Checking out the directory structure shows us that we have a new folder called tests, and in that a folder called client and a folder called server. That structure allows us to keep tests separate from source code and server tests separate from client tests. Inside the server tests you will see `test.js`, a file with everything commented out right now. Before we start writing tests, let's make sure that mocha is working in the first place. When we installed mocha, it gave us a binary file that will run our tests with the location of our tests as a command line argument. We can run our server tests with `./node_modules/mocha/bin/mocha tests/server`, which should give output like `0 passing (2ms)`, which makes sense because we don't have any tests written.

Time to write some tests then. In `test.js`, uncomment up through line 5, so you have:
```// Setup our assertion library
var expect = require('chai').expect;

var index = require('../../routes/index');```
The first line loads the [Chai](http://chaijs.com/) assertion library and assigns it's 'expect' test syntax to the variable `expect`, which we will use to write tests. The second line just loads our index route, because we will want to test that. If we run our tests again, we should still see nothing passing, but no errors. Now uncomment up to line 17, as well as line 22, leaving you with:
```
describe("A test suite", function() {
	// Syncronous
	it('should pass', function() { 
		expect(true).to.be.true; 
	});
});
```
Now running the tests should tell us that 1 test is passing! Let's take a look at what we just did. First off, we call the `describe` function, which creates our test suite. It takes a string, which servers as the name of the test suite, as well as a function with no arguments that runs the tests in the suite. Next we call `it`, which sets up a test for some piece of functionality, taking first the name of that test, and then a function, in this case with no arguments. Inside that function we finally make an assertion using `expect`, which we loaded from chai earlier. `expect` takes some value and returns an object with a bunch of useful comparison methods that will compare to the object we passed in. In this case, `expect(true).to.be.true` means, in English, "Take the value 'true' and see if it is truthy" which, of course, it is. The syntax here probably feels a little weird, and in fact t is. the `to` and `be` methods actually do nothing, they just chain together because somebody, sometime decided it was a good idea to be able to write test assertions that sound like English. We happen to agree with them, but if you do not, check out Chai's other assertion syntaxes and find one you like, they all have the same functionality. `true` is what actually does a comparison. All you really need to know is that it checks what comes before it to see if it is truthy, and if so it passes the test and otherwise it fails, and mocha takes care of the details of reporting that out. Every asserion you write will start with a call to `expect`, have some chaining methods (or maybe none), and some comparison method at the end. Chai's assertion methods are documented [here](http://chaijs.com/api/bdd/).

This is all great for synchronous testing, but how do we deal with asynchronicity? We need some way to tell mocha to wait for our callbacks to run and assertions in them to fire before it finishes testing. We can do that in our call to `it` when setting up our tests. The second argument to `it` is a function, and that function can have 0 or 1 arguments. In the 0 argument case, it does synchronous tests like above. In the 1 argument case, however, your `it` function receives as an argument another function, idiomatically called `done`. Now, mocha will wait until `done` is called before it proceeds with more testing, allowing you to call `done` after running assertions in a callback to run async tests. Uncomment lines 15-21 and you can see this in action. Running our tests now shows 2 passing, one of which takes about a second, showing that our async test is working. Async tests aren't that complicated to do in mocha, but they are a little dangerous. If `done` never gets called for some reason (like an error in your callback), mocha will not know to stop waiting for it, and your tests will hang. To prevent haning, mocha has a default timeout of 2 seconds on async tests, after which they will fail